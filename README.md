# PerfLLM
Large Language Models (LLMs) are increasingly being applied across various fields. Systems designed for serving LLMs are rapidly emerging. These systems introduce a variety of techniques to enhancing the performance of LM serving, including time to first token(TTFT), time per output token(TPOT) and throughput(tokens per second). However, the lack of benchmarks comparing emerging LLM serving systems under various scenarios makes it challenging to identify performance bottlenecks and opportunities for further optimization. Additionally, the inadequacy of evaluation metrics for LLM serving systems further exacerbates these issues.

To address this gap, We provide the PerfLLM benchmark suite, which consists of two components: Generator and Evaluator. We characterize the features of LLMs workloads as corpus distribution and concurrency. In the Generator of our PerfLLM benchmark suite, these features are produced by corpus and concurrency generator, respectively. Both components support the generation of workloads in real-world and dummy modes, significantly expanding the range of workloads that can be produced and enhancing ease of use. Our evaluator has two main responsibilities, recording the workload trace and reporting comprehensive benchmark results, including metrics such as efficiency, fairness, and stability. Addtional, we have collected a core set of datasets and benchmarks for two wisedly used systems: vLLM and TensorRT-LLM. We hope that the results from our benchmark can provide insights for system optimization and that our PerfLLM will foster the development of additional LLM serving systems and workloads.


